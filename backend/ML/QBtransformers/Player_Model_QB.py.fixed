import pandas as pd
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import os
import matplotlib.pyplot as plt
import seaborn as sns

# Check device stability
DEVICE = torch.device('cpu')
print(f"Using device: {DEVICE}")

# --- PFF CLASS DEFINITIONS (3 TIERS) ---
def get_tier(grade):
    if grade >= 80.0: return 2
    elif grade >= 60.0: return 1
    else: return 0

TIER_NAMES = ["Reserve/Poor", "Starter/Average", "Elite/High Quality"]

# 1. Time2Vec Layer (PyTorch Implementation)
class Time2Vec(nn.Module):
    def __init__(self, input_dim, kernel_size=1):
        super(Time2Vec, self).__init__()
        self.k = kernel_size
        self.input_dim = input_dim
        self.w0 = nn.Parameter(torch.randn(input_dim, 1)) 
        self.b0 = nn.Parameter(torch.randn(input_dim, 1))
        self.wk = nn.Parameter(torch.randn(input_dim, kernel_size))
        self.bk = nn.Parameter(torch.randn(input_dim, kernel_size))
        
    def forward(self, x):
        x_uns = x.unsqueeze(-1)
        linear = x_uns * self.w0 + self.b0
        periodic = torch.sin(x_uns * self.wk + self.bk)
        out = torch.cat([linear, periodic], dim=-1)
        out = out.reshape(x.size(0), x.size(1), -1)
        return out

# 2. Transformer Classification Model
class PlayerTransformerClassifier(nn.Module):
    def __init__(self, input_dim, seq_len, num_classes=3, kernel_size=1, num_heads=4, ff_dim=64, num_layers=2, dropout=0.1):
        super(PlayerTransformerClassifier, self).__init__()
        self.time2vec = Time2Vec(input_dim, kernel_size)
        self.embed_dim = input_dim * (kernel_size + 1)
        
        if self.embed_dim % num_heads != 0:
            new_dim = (self.embed_dim // num_heads + 1) * num_heads
            self.pad_proj = nn.Linear(self.embed_dim, new_dim)
            self.embed_dim = new_dim
        else:
            self.pad_proj = nn.Identity()

        encoder_layer = nn.TransformerEncoderLayer(
            d_model=self.embed_dim, 
            nhead=num_heads, 
            dim_feedforward=ff_dim, 
            dropout=dropout,
            batch_first=True
        )
        self.pos_embedding = nn.Parameter(torch.randn(1, seq_len + 1, self.embed_dim))
        self.cls_token = nn.Parameter(torch.randn(1, 1, self.embed_dim))
        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
        
        self.classifier = nn.Sequential(
            nn.Linear(self.embed_dim, 64),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(64, num_classes)
        )
        
    def forward(self, x, mask=None):
        x = self.time2vec(x)
        x = self.pad_proj(x)
        batch_size = x.size(0)
        cls_tokens = self.cls_token.expand(batch_size, -1, -1)
        x = torch.cat((cls_tokens, x), dim=1)
        if mask is not None:
            cls_mask = torch.zeros((batch_size, 1), dtype=torch.bool, device=x.device)
            full_mask = torch.cat([cls_mask, mask], dim=1)
        else:
            full_mask = None
        x = x + self.pos_embedding
        x = self.transformer_encoder(x, src_key_padding_mask=full_mask)
        x = x[:, 0, :]
        x = self.classifier(x)
        return x

# 3. Data Processing
data_path = '../../ML/QB.csv' 
if not os.path.exists(data_path):
    data_path = '/Users/pranaynandkeolyar/Documents/NFLSalaryCap/backend/ML/QB.csv'

print(f"Reading data from: {data_path}")
df = pd.read_csv(data_path)

# Handle Data Issues
df['adjusted_value'] = pd.to_numeric(df['adjusted_value'], errors='coerce')

# Filter backups
MIN_DROPBACKS = 100
print(f"Filtering for Minimum Dropbacks: {MIN_DROPBACKS}")
if 'dropbacks' in df.columns:
    df = df[df['dropbacks'] >= MIN_DROPBACKS].copy()

if 'Year' not in df.columns and 'year' in df.columns: 
    df.rename(columns={'year': 'Year'}, inplace=True)

if 'player' not in df.columns:
     raise KeyError(f"Could not find 'player' column.")

print(f"Unique Players: {df['player'].nunique()}")

MAX_SEQUENCE_LENGTH = 5 

# --- ADVANCED FEATURE ENGINEERING ---
print("Applying Feature Engineering...")
df.sort_values(by=['player', 'Year'], inplace=True)
df["years_in_league"] = df.groupby("player").cumcount()
df["delta_grade"] = df.groupby("player")["grades_offense"].diff().fillna(0)
df["delta_epa"]   = df.groupby("player")["Net EPA"].diff().fillna(0)
df["delta_btt"]   = df.groupby("player")["btt_rate"].diff().fillna(0)
df['team_performance_proxy'] = df.groupby(['Team', 'Year'])['Net EPA'].transform('mean')

features = [
    'grades_pass', 'grades_offense', 'qb_rating', 'adjusted_value',
    'Cap_Space', 'ypa', 'twp_rate', 'btt_rate', 'completion_percent',
    'years_in_league', 'delta_grade', 'delta_epa', 'delta_btt',
    'team_performance_proxy'
]
target_col = 'grades_offense' 

available_features = [f for f in features if f in df.columns]
if len(available_features) < len(features):
    features = available_features

df_clean = df.dropna(subset=features + [target_col])
print(f"Cleaned Data Size: {len(df_clean)}")

# Create Classes BEFORE normalization
df_clean['tier'] = df_clean[target_col].apply(get_tier)
print("Target Class Distribution:")
print(df_clean['tier'].value_counts().sort_index())

# Normalize Features
scaler = StandardScaler()
df_clean[features] = scaler.fit_transform(df_clean[features])

import joblib
scaler_path = 'player_scaler.joblib'
joblib.dump(scaler, scaler_path)
print(f"Scaler saved to {scaler_path}")

def create_sequences_variable(dataset, max_seq_len, features, target_class_col):
    X, y, masks, years = [], [], [], []
    for player, group in dataset.groupby('player'):
        group = group.sort_values('Year')
        vals = group[features].values
        targs = group[target_class_col].values
        yr_vals = group['Year'].values
        for i in range(1, len(group)):
            start_idx = max(0, i - max_seq_len)
            history_vals = vals[start_idx:i]
            actual_len = len(history_vals)
            padding_len = max_seq_len - actual_len
            if padding_len > 0:
                pad = np.zeros((padding_len, len(features)))
                padded_x = np.vstack([pad, history_vals])
                mask = [True] * padding_len + [False] * actual_len
            else:
                padded_x = history_vals
                mask = [False] * actual_len
            X.append(padded_x)
            y.append(targs[i])
            masks.append(mask)
            years.append(yr_vals[i])
    return (np.array(X, dtype=np.float32), np.array(y, dtype=np.int64), 
            np.array(masks, dtype=bool), np.array(years))

# --- TIME-BASED SPLIT ---
X_all, y_all, masks_all, years_all = create_sequences_variable(df_clean, MAX_SEQUENCE_LENGTH, features, 'tier')
train_mask = years_all <= 2021
test_mask = years_all == 2022
if train_mask.sum() == 0 or test_mask.sum() == 0:
    latest_year = years_all.max()
    test_mask = years_all == latest_year
    train_mask = years_all < latest_year

X_train, y_train, masks_train = X_all[train_mask], y_all[train_mask], masks_all[train_mask]
X_test, y_test, masks_test = X_all[test_mask], y_all[test_mask], masks_all[test_mask]

# Class Balance & Weights
unique, counts = np.unique(y_train, return_counts=True)
class_counts = dict(zip(unique, counts))
total_samples = len(y_train)
class_weights = [total_samples / (3 * class_counts.get(i, 1)) for i in range(3)]
class_weights_tensor = torch.tensor(class_weights, dtype=torch.float32).to(DEVICE)
print(f"Train Class Dist: {class_counts}")
print(f"Class Weights: {class_weights}")

# Dataset & DataLoader
class PlayerDataset(Dataset):
    def __init__(self, X, y, masks):
        self.X = torch.tensor(X, dtype=torch.float32)
        self.y = torch.tensor(y, dtype=torch.int64)
        self.masks = torch.tensor(masks, dtype=torch.bool)
    def __len__(self): return len(self.X)
    def __getitem__(self, idx): return self.X[idx], self.y[idx], self.masks[idx]

train_loader = DataLoader(PlayerDataset(X_train, y_train, masks_train), batch_size=32, shuffle=True)
test_loader = DataLoader(PlayerDataset(X_test, y_test, masks_test), batch_size=32, shuffle=False)

# Train
model = PlayerTransformerClassifier(input_dim=len(features), seq_len=MAX_SEQUENCE_LENGTH).to(DEVICE)
criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=10)

EPOCHS = 150
best_acc = 0.0
for epoch in range(EPOCHS):
    model.train()
    for inputs, targets, masks in train_loader:
        optimizer.zero_grad()
        loss = criterion(model(inputs, mask=masks), targets)
        loss.backward()
        optimizer.step()
    model.eval()
    val_correct, val_total = 0, 0
    with torch.no_grad():
        for inputs, targets, masks in test_loader:
            outputs = model(inputs, mask=masks)
            _, predicted = torch.max(outputs.data, 1)
            val_total += targets.size(0)
            val_correct += (predicted == targets).sum().item()
    val_acc = val_correct / val_total
    scheduler.step(val_acc)
    if val_acc > best_acc:
        best_acc = val_acc
        torch.save(model.state_dict(), 'best_classifier.pth')
    if (epoch+1) % 20 == 0:
        print(f"Epoch {epoch+1}, Val Acc: {val_acc:.4f}")

# Evaluation
model.load_state_dict(torch.load('best_classifier.pth'))
model.eval()
all_preds, all_targets = [], []
with torch.no_grad():
    for inputs, targets, masks in test_loader:
        outputs = model(inputs, mask=masks)
        _, predicted = torch.max(outputs.data, 1)
        all_preds.extend(predicted.cpu().numpy())
        all_targets.extend(targets.cpu().numpy())

print(f"\nFinal Test Accuracy: {accuracy_score(all_targets, all_preds):.4f}")
print(classification_report(all_targets, all_preds, target_names=[TIER_NAMES[i] for i in sorted(list(set(all_targets)))], digits=4))
